Python 3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:39) [Clang 13.0.0 (clang-1300.0.29.30)] on darwin
Type "help", "copyright", "credits" or "license()" for more information.
import streamlit as st
import requests
import time
import tiktoken
import pandas as pd
import matplotlib.pyplot as plt


BASE_URL = "http://127.0.0.1:8000"
def count_tokens(text, model):
    
    try:
        if model in ["GPT-4o", "GPT-3.5"]:
            # ‚úÖ Use OpenAI tokenizer for GPT models
            enc = tiktoken.encoding_for_model("gpt-4")
            return len(enc.encode(text))
        elif model in ["Claude", "Claude-3"]:
            # ‚úÖ Approximate Claude tokenization (word count * 1.2)
            return int(len(text.split()) * 1.2)
        elif model in ["Gemini-Flash", "Gemini-Pro"]:
            # ‚úÖ Approximate Gemini tokenization (word count * 1.15)
            return int(len(text.split()) * 1.15)
        elif model in ["DeepSeek"]:
            # ‚úÖ Approximate DeepSeek tokenization (word count * 1.1)
            return int(len(text.split()) * 1.1)
        elif model in ["Grok"]:
            # ‚úÖ Approximate Grok tokenization (word count * 1.05)
            return int(len(text.split()) * 1.05)
        else:
            # Fallback: General word count estimation
            return len(text.split())
    except Exception:
        return len(text.split())  
# ‚úÖ Define pricing per 1M tokens for each LLM (in USD)
MODEL_PRICING = {
    "GPT-4o": {"input_price": 5.00, "output_price": 10.00}, 
    "Claude": {"input_price": 3.00, "output_price": 15.00},
    "Gemini-Flash": {"input_price": 0.10, "output_price": 0.40},
    "DeepSeek": {"input_price": 0.27, "output_price": 1.10},
    "Grok": {"input_price": 2.00, "output_price": 10.00}
}

# Page Configuration
st.set_page_config(page_title="Document AI Processing", layout="centered")

# Page Title
st.markdown("<h1 style='text-align: center; color: Black;'>AI Document Processing Platform</h1>", unsafe_allow_html=True)

# 1Ô∏è‚É£ **Model Selection**
st.markdown("<h2 style='text-align: center; color: Black;'>Select Model</h2>", unsafe_allow_html=True)
model = st.selectbox("Choose a model", ["GPT-4o", "Gemini-Flash", "DeepSeek", "Claude", "Grok"], key="model")

# 2Ô∏è‚É£ **Upload a PDF First**
st.markdown("<h2 style='text-align: center; color: Black;'>Upload a PDF</h2>", unsafe_allow_html=True)
uploaded_file = st.file_uploader("Upload a PDF üìÑ", type="pdf")

if uploaded_file and st.button("Process PDF üìÇ", use_container_width=True):
    with st.spinner("üìÉ Extracting text from PDF..."):
        files = {
        "file": (uploaded_file.name, uploaded_file.getvalue(), "application/pdf")
    }
    response = requests.post(f"{BASE_URL}/upload_pdf/", files=files)


    if response.status_code == 200:
            scraped_file = response.json()["filename"]
            st.success(f"PDF processed successfully! Extracted content saved as `{scraped_file}`")
    else:
            st.error(f" Upload failed: {response.text}")

# 3Ô∏è‚É£ **List Available Processed Files**
st.markdown("<h2 style='text-align: center; color: Black;'>Select Processed File</h2>", unsafe_allow_html=True)

response = requests.get(f"{BASE_URL}/select_pdfcontent/")

if response.status_code == 200:
    markdown_files = response.json().get("markdowns", [])
    
    if markdown_files:
        selected_markdown = st.selectbox("Select a Processed Markdown File üìÇ", markdown_files)
    else:
        st.warning("‚ö†Ô∏è No processed Markdown files found. Upload a PDF first.")
        selected_markdown = None
else:
    st.error("‚ùå Failed to fetch processed Markdown files.")
    selected_markdown = None

# 4Ô∏è‚É£ **Summarization Section**
# ‚úÖ Ensure session state has "summary" key
if "summary" not in st.session_state:
    st.session_state["summary"] = None

# Summarization Section
st.markdown("<h2 style='text-align: center; color: black;'>Summarization</h2>", unsafe_allow_html=True)

if selected_markdown and st.button("Summarize Document ", use_container_width=True):
    with st.spinner("Summarizing document... Please wait."):
        response = requests.post(
            f"{BASE_URL}/summarize/",
            data={"pdf_name": selected_markdown, "llm": model}
        )

        if response.status_code == 200:
            task_id = response.json().get("task_id")
            elapsed_time = 0
            max_wait_time = 600  # 10 minutes timeout

            while elapsed_time < max_wait_time:
                time.sleep(2)  # Reduce API load
                elapsed_time += 2
                result_response = requests.get(f"{BASE_URL}/get_result/{task_id}")

                try:
                    result_data = result_response.json()
                    if "result" in result_data:
                        # ‚úÖ Store summary in session state
                        st.session_state["summary"] = result_data["result"]
                        break
                except requests.exceptions.JSONDecodeError:
                    st.error(" Failed to retrieve a valid response from the server.")
                    break
            else:
                st.error(" Summarization took too long. Please try again later.")
        else:
            st.error(f" Summarization request failed: {response.text}")

# ‚úÖ Display stored summary even after refresh
if st.session_state["summary"]:
    st.markdown("<h3 style='text-align: center; color: black;'>Summarization Result:</h3>", unsafe_allow_html=True)
    st.write(st.session_state["summary"])
# ‚úÖ Ensure session state has "answer" key
if "answer" not in st.session_state:
    st.session_state["answer"] = None

st.markdown("<h2 style='text-align: center; color: black;'>Ask a Question</h2>", unsafe_allow_html=True)

# ‚úÖ Use st.text_input() instead of st.text_area() so Enter submits automatically
question = st.text_input("Enter your question and press Enter:")

# ‚úÖ Create an empty placeholder for answer display
answer_placeholder = st.empty()

if selected_markdown and question and st.button("Get Answer üí°", use_container_width=True):
    with st.spinner("Fetching answer... Please wait."):
        response = requests.post(
            f"{BASE_URL}/ask_question/",
            data={"pdf_name": selected_markdown, "llm": model, "question": question}
        )

        if response.status_code == 200:
            task_id = response.json().get("task_id")

            while True:
                time.sleep(2)
                result_response = requests.get(f"{BASE_URL}/get_result/{task_id}")
                result_data = result_response.json()

                if "result" in result_data:
                    # ‚úÖ Store answer in session state
                    st.session_state["answer"] = result_data["result"]
                    break
        else:
            st.error(f" Failed to submit question request: {response.text}")

# ‚úÖ Display stored answer without affecting summary
if st.session_state["answer"]:

    answer_placeholder.write(st.session_state["answer"])

if selected_markdown:
    # Fetch the file content
    file_response = requests.get(f"{BASE_URL}/download_markdown/{selected_markdown}")

    if file_response.status_code == 200:
        file_content = file_response.text
        pdf_token_count = count_tokens(file_content, model)  # ‚úÖ Dynamic token count
        st.session_state["pdf_token_count"] = pdf_token_count
    else:
        st.session_state["pdf_token_count"] = 0
        st.error(" Failed to fetch file content.")

# ‚úÖ Display token count for the selected PDF
if "pdf_token_count" in st.session_state:
    st.markdown(f" **Token Count for Selected PDF**: {st.session_state['pdf_token_count']} tokens")

# ‚úÖ Display stored summary even after refresh
if st.session_state["summary"]:
    summary_token_count = count_tokens(st.session_state["summary"], model)  # ‚úÖ Model-aware token count
    st.session_state["summary_token_count"] = summary_token_count
    st.markdown(f" **Token Count for Summarization**: {st.session_state['summary_token_count']} tokens")

if question:
    question_token_count = count_tokens(question, model)  # ‚úÖ Count question tokens per model
    st.session_state["question_token_count"] = question_token_count
    st.markdown(f" **Token Count for Question**: {st.session_state['question_token_count']} tokens")

if st.session_state["answer"]:
    answer_token_count = count_tokens(st.session_state["answer"], model)  # ‚úÖ Count answer tokens per model
    st.session_state["answer_token_count"] = answer_token_count

    answer_placeholder.write(st.session_state["answer"])
    st.markdown(f" **Token Count for Answer**: {st.session_state['answer_token_count']} tokens")

if selected_markdown and model:
    # Fetch the file content only when both file and model are selected
    file_response = requests.get(f"{BASE_URL}/download_markdown/{selected_markdown}")

    if file_response.status_code == 200:
        file_content = file_response.text
        pdf_token_count = count_tokens(file_content, model)  # ‚úÖ Dynamic token count
        st.session_state["pdf_token_count"] = pdf_token_count
    else:
        st.session_state["pdf_token_count"] = 0
        st.error(" Failed to fetch file content.")

    # ‚úÖ Display token count and cost for the selected PDF
    pdf_price = (st.session_state["pdf_token_count"] / 1_000_000) * MODEL_PRICING[model]["input_price"]
    st.markdown(f" **Cost for pdf:** ${pdf_price:.5f}")


if question and model:
    question_token_count = count_tokens(question, model)  # ‚úÖ Count question tokens per model
    st.session_state["question_token_count"] = question_token_count
    question_price = (question_token_count / 1_000_000) * MODEL_PRICING[model]["input_price"]
    
    st.markdown(f"**Cost for Question:** ${question_price:.5f}")

# ‚úÖ Display stored summary even after refresh
if st.session_state["summary"]:
    summary_token_count = count_tokens(st.session_state["summary"], model)  # ‚úÖ Model-aware token count
    st.session_state["summary_token_count"] = summary_token_count
    summary_price = (summary_token_count / 1_000_000) * MODEL_PRICING[model]["output_price"]
    st.markdown(f"**Cost for Summarisation:** ${summary_price:.5f}")


if st.session_state["answer"]:
    answer_token_count = count_tokens(st.session_state["answer"], model)  # ‚úÖ Count answer tokens per model
    st.session_state["answer_token_count"] = answer_token_count
    answer_price = (answer_token_count / 1_000_000) * MODEL_PRICING[model]["output_price"]

    answer_placeholder.write(st.session_state["answer"])
    st.markdown(f"**Cost for Answer:** ${answer_price:.5f}")

if "pdf_token_count" in st.session_state and "question_token_count" in st.session_state and model:
    total_input_tokens = st.session_state["pdf_token_count"] + st.session_state["question_token_count"]
    total_input_cost = (total_input_tokens / 1_000_000) * MODEL_PRICING[model]["input_price"]
else:
    total_input_tokens = 0
    total_input_cost = 0

if "summary_token_count" in st.session_state and "answer_token_count" in st.session_state and model:
    total_output_tokens = st.session_state["summary_token_count"] + st.session_state["answer_token_count"]
    total_output_cost = (total_output_tokens / 1_000_000) * MODEL_PRICING[model]["output_price"]
else:
    total_output_tokens = 0
    total_output_cost = 0

import pandas as pd  # Ensure pandas is imported

if model and selected_markdown:  # ‚úÖ Ensure both model and file are selected
    total_cost = total_input_cost + total_output_cost

    # ‚úÖ Replace "-" with 0 for numerical values to prevent errors
    cost_data = pd.DataFrame({
        "Category": [" Input Tokens (PDF + Question)", "Output Tokens (Summarization + Answer)", "Total Cost"],
        "Token Count": [total_input_tokens if total_input_tokens > 0 else 0, 
                        total_output_tokens if total_output_tokens > 0 else 0, 
                        None],  # Keep "None" for total cost to avoid errors
        "Cost (USD)": [total_input_cost if total_input_cost > 0 else 0, 
                       total_output_cost if total_output_cost > 0 else 0, 
                       total_cost if total_cost > 0 else 0]  # Ensure numerical values
    })

    # ‚úÖ Convert "Token Count" to integers to avoid serialization error
    cost_data["Token Count"] = cost_data["Token Count"].astype("Int64")

    # ‚úÖ Display table in Streamlit
    st.markdown("### **Total Token Usage & Cost (Based on 1M Token Pricing)**")
    st.table(cost_data)


... # ‚úÖ Create a bar chart for token usage
... if selected_markdown and model:
...     fig, ax = plt.subplots(figsize=(8, 5))
...     labels = ["PDF Tokens", "Summary Tokens", "Question Tokens", "Answer Tokens"]
...     values = [
...         st.session_state.get("pdf_token_count", 0),
...         st.session_state.get("summary_token_count", 0),
...         st.session_state.get("question_token_count", 0),
...         st.session_state.get("answer_token_count", 0),
...     ]
...     ax.bar(labels, values)
...     ax.set_ylabel("Token Count")
...     ax.set_title("Token Usage Breakdown")
... 
...     # ‚úÖ Display the chart in Streamlit
...     st.pyplot(fig)
... 
... # ‚úÖ Reset stored values when the user changes the model or file
... if "prev_model" not in st.session_state:
...     st.session_state["prev_model"] = model
... 
... if "prev_file" not in st.session_state:
...     st.session_state["prev_file"] = selected_markdown
... 
... # ‚úÖ If the user selects a new model or file, reset session state
... if model != st.session_state["prev_model"] or selected_markdown != st.session_state["prev_file"] or st.session_state["prev_question"] != question:
...     st.session_state["summary"] = None
...     st.session_state["answer"] = None
...     st.session_state["question"] = ""
...     st.session_state["question_token_count"] = 0
...     st.session_state["summary_token_count"] = 0
...     st.session_state["answer_token_count"] = 0
...     st.session_state["pdf_token_count"] = 0
...     st.session_state["prev_model"] = model
...     st.session_state["prev_file"] = selected_markdown
...     st.session_state["prev_question"] = question
...     # ‚úÖ Force a rerun of the Streamlit app to reflect the reset state
